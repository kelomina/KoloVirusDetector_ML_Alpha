import os
import argparse
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import joblib
import yaml
import logging
from datetime import datetime
import pickle
import torch
from tqdm import tqdm

from data_processing import load_dataset, load_single_file, FeatureEngineer, extract_all_features
from models import MalwareDetector, DeepMalwareDetector
from evaluation import ModelEvaluator
from feature_extraction import extract_api_sequence, calculate_api_importance_scores, get_api_sequences_from_files
from feature_extraction.byte_sequence import extract_byte_sequences, extract_bytes_from_file
from feature_extraction.image_features import generate_pe_images, generate_pe_image

# 设置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('MalwareDetector')

def train_lightgbm_model(config):
    """
    训练LightGBM恶意软件检测模型
    
    Args:
        config: 配置字典
    """
    # 创建输出目录
    output_dir = config.get('output_dir', 'output')
    os.makedirs(output_dir, exist_ok=True)
    
    # 设置模型路径
    model_path = os.path.join(output_dir, 'malware_detector.pkl')
    feature_importance_path = os.path.join(output_dir, 'feature_importance.csv')
    
    # 加载数据集，如果已经有处理好的特征则直接加载
    features_path = config.get('features_path')
    if features_path and os.path.exists(features_path):
        logger.info(f"从文件加载特征: {features_path}")
        with open(features_path, 'rb') as f:
            data = pickle.load(f)
            X = data['X']
            y = data['y']
    else:
        # 从原始数据中提取特征
        benign_dir = config.get('benign_dir')
        malware_dir = config.get('malware_dir')
        
        if not benign_dir or not malware_dir:
            raise ValueError("必须提供benign_dir和malware_dir路径")
        
        limit = config.get('sample_limit')
        n_jobs = config.get('n_jobs', 4)
        
        logger.info("开始从文件中提取特征...")
        features_save_path = os.path.join(output_dir, 'features.pkl')
        X, y = load_dataset(benign_dir, malware_dir, features_save_path, n_jobs, limit)
    
    # 数据分割
    test_size = config.get('test_size', 0.2)
    random_state = config.get('random_state', 42)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    logger.info(f"训练集大小: {X_train.shape}, 测试集大小: {X_test.shape}")
    
    # 特征工程
    logger.info("执行特征工程...")
    feature_engineer = FeatureEngineer(top_features=config.get('top_features', 500))
    X_train_fe, y_train_fe = feature_engineer.fit_transform(X_train, y_train)
    X_test_fe = feature_engineer.transform(X_test)
    
    # 保存特征工程器
    joblib.dump(feature_engineer, os.path.join(output_dir, 'feature_engineer.pkl'))
    
    # 模型训练
    logger.info("开始训练模型...")
    
    # 读取LightGBM参数
    lgb_params = config.get('lgb_params', {})
    n_estimators = config.get('n_estimators', 1000)
    early_stopping_rounds = config.get('early_stopping_rounds', 50)
    
    model = MalwareDetector(params=lgb_params, n_estimators=n_estimators, 
                            early_stopping_rounds=early_stopping_rounds)
    
    # 是否进行参数优化
    if config.get('optimize_params', False):
        logger.info("正在执行超参数优化...")
        n_trials = config.get('n_trials', 100)
        best_params = model.optimize_hyperparameters(X_train_fe, y_train_fe, n_trials=n_trials)
        logger.info(f"最佳参数: {best_params}")
    
    # 训练模型
    metrics = model.train(X_train_fe, y_train_fe)
    logger.info(f"训练完成，训练指标: {metrics}")
    
    # 保存模型
    model.save_model(model_path, feature_importance_path)
    logger.info(f"模型已保存到: {model_path}")
    
    # 模型评估
    logger.info("评估模型性能...")
    evaluator = ModelEvaluator(model, X_test_fe, y_test)
    eval_report = evaluator.generate_report(output_dir)
    
    logger.info("模型评估指标:")
    for metric_name, metric_value in eval_report.items():
        logger.info(f"{metric_name}: {metric_value:.4f}")
    
    return model, feature_engineer, evaluator

def predict_lightgbm_file(model_path, feature_engineer_path, file_path):
    """
    使用LightGBM模型对单个文件进行预测
    
    Args:
        model_path: 模型文件路径
        feature_engineer_path: 特征工程器路径
        file_path: 待预测文件路径
    """
    # 加载模型和特征工程器
    model = MalwareDetector.load_model(model_path)
    feature_engineer = joblib.load(feature_engineer_path)
    
    # 提取特征
    logger.info(f"从文件提取特征: {file_path}")
    X = load_single_file(file_path)
    
    if X is None:
        logger.error(f"提取特征失败: {file_path}")
        return None
    
    # 应用特征工程
    X_fe = feature_engineer.transform(X)
    
    # 预测
    pred, prob = model.predict(X_fe)
    
    result = {
        'file': file_path,
        'prediction': 'Malware' if pred[0] else 'Benign',
        'probability': float(prob[0])
    }
    
    logger.info(f"预测结果: {result}")
    return result

def train_deep_model(config):
    """
    训练深度学习恶意软件检测模型
    
    Args:
        config: 配置字典
    """
    # 创建输出目录
    output_dir = config.get('output_dir', 'output')
    os.makedirs(output_dir, exist_ok=True)
    
    # 设置模型路径
    model_path = os.path.join(output_dir, 'deep_malware_detector.pt')
    swa_model_path = os.path.join(output_dir, 'deep_malware_detector_swa.pt')
    
    # 读取多模态设置
    use_byte_seq = config.get('use_byte_sequence', True)
    use_image = config.get('use_image', True)
    byte_seq_max_length = config.get('byte_sequence_max_length', 2048)
    image_width = config.get('image_width', 32)
    image_max_bytes = config.get('image_max_bytes', 4096)
    
    # 加载数据集，如果已经有处理好的特征则直接加载
    features_path = config.get('features_path')
    if features_path and os.path.exists(features_path):
        logger.info(f"从文件加载特征: {features_path}")
        with open(features_path, 'rb') as f:
            data = pickle.load(f)
            X = data['X']
            y = data['y']
            
        # 收集文件路径，用于提取多模态特征
        benign_dir = config.get('benign_dir')
        malware_dir = config.get('malware_dir')
        
        if not benign_dir or not malware_dir:
            raise ValueError("必须提供benign_dir和malware_dir路径")
            
        # 收集文件路径
        benign_files = []
        for root, _, files in os.walk(benign_dir):
            for file in files:
                if file.lower().endswith(('.exe', '.dll', '.sys')):
                    benign_files.append(os.path.join(root, file))
        
        malware_files = []
        for root, _, files in os.walk(malware_dir):
            for file in files:
                if file.lower().endswith(('.exe', '.dll', '.sys')):
                    malware_files.append(os.path.join(root, file))
                    
        # 应用限制
        limit = config.get('sample_limit')
        if limit:
            benign_files = benign_files[:limit]
            malware_files = malware_files[:limit]
            
        all_files = benign_files + malware_files
            
        # 加载或提取API序列
        api_sequence_path = config.get('api_sequence_path')
        if api_sequence_path and os.path.exists(api_sequence_path):
            logger.info(f"从文件加载API序列: {api_sequence_path}")
            with open(api_sequence_path, 'rb') as f:
                api_sequences = pickle.load(f)
        else:
            # 提取API序列
            logger.info("从文件中提取API序列...")
            benign_sequences = get_api_sequences_from_files(benign_files)
            malware_sequences = get_api_sequences_from_files(malware_files)
            
            api_sequences = benign_sequences + malware_sequences
            
            # 保存API序列
            api_sequence_save_path = os.path.join(output_dir, 'api_sequences.pkl')
            with open(api_sequence_save_path, 'wb') as f:
                pickle.dump(api_sequences, f)
            logger.info(f"API序列保存到: {api_sequence_save_path}")
            
        # 加载或提取字节序列特征
        byte_sequences = None
        if use_byte_seq:
            byte_sequence_path = config.get('byte_sequence_path')
            if byte_sequence_path and os.path.exists(byte_sequence_path):
                logger.info(f"从文件加载字节序列: {byte_sequence_path}")
                byte_sequences = np.load(byte_sequence_path, allow_pickle=True)
            else:
                # 提取字节序列
                logger.info("从文件中提取字节序列...")
                byte_sequences = extract_byte_sequences(all_files, max_length=byte_seq_max_length)
                
                # 保存字节序列
                byte_sequence_save_path = os.path.join(output_dir, 'byte_sequences.npy')
                np.save(byte_sequence_save_path, byte_sequences)
                logger.info(f"字节序列保存到: {byte_sequence_save_path}")
        
        # 加载或提取图像特征
        images = None
        if use_image:
            image_path = config.get('image_path')
            if image_path and os.path.exists(image_path):
                logger.info(f"从文件加载图像特征: {image_path}")
                images = np.load(image_path, allow_pickle=True)
            else:
                # 提取图像特征
                logger.info("从文件中生成图像特征...")
                images = generate_pe_images(all_files, width=image_width, max_bytes=image_max_bytes)
                
                # 保存图像特征
                image_save_path = os.path.join(output_dir, 'images.npy')
                np.save(image_save_path, images)
                logger.info(f"图像特征保存到: {image_save_path}")
    else:
        # 从原始数据中提取特征
        benign_dir = config.get('benign_dir')
        malware_dir = config.get('malware_dir')
        
        if not benign_dir or not malware_dir:
            raise ValueError("必须提供benign_dir和malware_dir路径")
        
        limit = config.get('sample_limit')
        n_jobs = config.get('n_jobs', 4)
        
        logger.info("开始从文件中提取特征...")
        features_save_path = os.path.join(output_dir, 'features.pkl')
        X, y = load_dataset(benign_dir, malware_dir, features_save_path, n_jobs, limit)
        
        # 收集文件路径
        benign_files = []
        for root, _, files in os.walk(benign_dir):
            for file in files:
                if file.lower().endswith(('.exe', '.dll', '.sys')):
                    benign_files.append(os.path.join(root, file))
        
        malware_files = []
        for root, _, files in os.walk(malware_dir):
            for file in files:
                if file.lower().endswith(('.exe', '.dll', '.sys')):
                    malware_files.append(os.path.join(root, file))
                    
        # 应用限制
        if limit:
            benign_files = benign_files[:limit]
            malware_files = malware_files[:limit]
            
        all_files = benign_files + malware_files
            
        # 提取API序列
        logger.info("从文件中提取API序列...")
        benign_sequences = get_api_sequences_from_files(benign_files)
        malware_sequences = get_api_sequences_from_files(malware_files)
        
        api_sequences = benign_sequences + malware_sequences
        
        # 保存API序列
        api_sequence_save_path = os.path.join(output_dir, 'api_sequences.pkl')
        with open(api_sequence_save_path, 'wb') as f:
            pickle.dump(api_sequences, f)
        logger.info(f"API序列保存到: {api_sequence_save_path}")
        
        # 提取字节序列特征
        byte_sequences = None
        if use_byte_seq:
            logger.info("从文件中提取字节序列...")
            byte_sequences = extract_byte_sequences(all_files, max_length=byte_seq_max_length)
            
            # 保存字节序列
            byte_sequence_save_path = os.path.join(output_dir, 'byte_sequences.npy')
            np.save(byte_sequence_save_path, byte_sequences)
            logger.info(f"字节序列保存到: {byte_sequence_save_path}")
        
        # 提取图像特征
        images = None
        if use_image:
            logger.info("从文件中生成图像特征...")
            images = generate_pe_images(all_files, width=image_width, max_bytes=image_max_bytes)
            
            # 保存图像特征
            image_save_path = os.path.join(output_dir, 'images.npy')
            np.save(image_save_path, images)
            logger.info(f"图像特征保存到: {image_save_path}")
    
    # 数据分割
    test_size = config.get('test_size', 0.2)
    random_state = config.get('random_state', 42)
    
    # 创建一个包含所有特征的列表
    split_data = [X, y, api_sequences]
    split_names = ['X', 'y', 'api_sequences']
    
    # 添加可选特征
    if use_byte_seq and byte_sequences is not None:
        split_data.append(byte_sequences)
        split_names.append('byte_sequences')
    
    if use_image and images is not None:
        split_data.append(images)
        split_names.append('images')
    
    # 执行数据分割
    split_results = train_test_split(*split_data, test_size=test_size, random_state=random_state, stratify=y)
    
    # 整理分割结果
    split_dict = {}
    for i, name in enumerate(split_names):
        split_dict[f"{name}_train"] = split_results[i*2]
        split_dict[f"{name}_test"] = split_results[i*2+1]
    
    # 分配变量
    X_train, X_test = split_dict['X_train'], split_dict['X_test']
    y_train, y_test = split_dict['y_train'], split_dict['y_test']
    api_train, api_test = split_dict['api_sequences_train'], split_dict['api_sequences_test']
    
    # 可选特征
    byte_train = split_dict.get('byte_sequences_train', None)
    byte_test = split_dict.get('byte_sequences_test', None)
    img_train = split_dict.get('images_train', None) 
    img_test = split_dict.get('images_test', None)
    
    logger.info(f"训练集大小: {X_train.shape}, 测试集大小: {X_test.shape}")
    
    # PE特征工程 - 只选择数值型特征用于深度学习模型
    logger.info("执行PE特征工程...")
    feature_engineer = FeatureEngineer(top_features=config.get('top_features', 500))
    X_train_fe, y_train_fe = feature_engineer.fit_transform(X_train, y_train)
    X_test_fe = feature_engineer.transform(X_test)
    
    # 保存特征工程器
    joblib.dump(feature_engineer, os.path.join(output_dir, 'feature_engineer.pkl'))
    
    # 特征标准化
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_fe)
    X_test_scaled = scaler.transform(X_test_fe)
    
    # 保存标准化器
    joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))
    
    # 深度学习模型参数
    pe_feature_dim = X_train_scaled.shape[1]
    embedding_dim = config.get('embedding_dim', 64)
    hidden_dim = config.get('hidden_dim', 128)
    lstm_layers = config.get('lstm_layers', 2)
    fc_hidden = config.get('fc_hidden', [256, 128])
    dropout = config.get('dropout', 0.3)
    lr = config.get('learning_rate', 0.001)
    batch_size = config.get('batch_size', 32)
    epochs = config.get('epochs', 50)
    swa_start = config.get('swa_start', 30)
    
    # 创建模型
    logger.info("创建深度学习模型...")
    deep_model = DeepMalwareDetector(
        pe_feature_dim=pe_feature_dim,
        embedding_dim=embedding_dim,
        hidden_dim=hidden_dim,
        lstm_layers=lstm_layers,
        fc_hidden=fc_hidden,
        dropout=dropout,
        lr=lr,
        batch_size=batch_size,
        epochs=epochs,
        swa_start=swa_start,
        use_byte_seq=use_byte_seq,
        use_image=use_image,
        use_amp=config.get('use_amp', True)
    )
    
    # 训练模型
    logger.info("开始训练深度学习模型...")
    deep_model.train(
        pe_features=X_train_scaled, 
        api_sequences=api_train,
        labels=y_train,
        val_pe_features=X_test_scaled,
        val_api_sequences=api_test,
        val_labels=y_test,
        byte_sequences=byte_train,
        images=img_train,
        val_byte_sequences=byte_test,
        val_images=img_test,
        num_workers=config.get('num_workers', 4)
    )
    
    # 保存模型
    logger.info(f"保存模型到: {model_path}")
    deep_model.save_model(model_path, swa_model_path)
    
    # 模型评估
    logger.info("评估模型性能...")
    y_pred, y_pred_proba = deep_model.predict(
        pe_features=X_test_scaled,
        api_sequences=api_test,
        use_swa=True,  # 使用SWA模型进行预测
        byte_sequences=byte_test,
        images=img_test
    )
    
    # 计算评估指标
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
    
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_proba)
    
    # 打印评估指标
    logger.info("深度学习模型评估指标:")
    logger.info(f"Accuracy: {accuracy:.4f}")
    logger.info(f"Precision: {precision:.4f}")
    logger.info(f"Recall: {recall:.4f}")
    logger.info(f"F1 Score: {f1:.4f}")
    logger.info(f"AUC: {auc:.4f}")
    
    # 创建评估报告
    eval_report_path = os.path.join(output_dir, 'deep_model_evaluation.txt')
    with open(eval_report_path, 'w') as f:
        f.write('=== 深度学习恶意软件检测模型评估报告 ===\n')
        f.write(f'生成时间: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n\n')
        
        f.write('--- 性能指标 ---\n')
        f.write(f'准确率 (Accuracy): {accuracy:.4f}\n')
        f.write(f'精确率 (Precision): {precision:.4f}\n')
        f.write(f'召回率 (Recall): {recall:.4f}\n')
        f.write(f'F1分数 (F1-Score): {f1:.4f}\n')
        f.write(f'ROC曲线下面积 (AUC): {auc:.4f}\n\n')
        
        f.write('--- 模型配置 ---\n')
        f.write(f'PE特征维度: {pe_feature_dim}\n')
        f.write(f'嵌入维度: {embedding_dim}\n')
        f.write(f'隐藏层维度: {hidden_dim}\n')
        f.write(f'LSTM层数: {lstm_layers}\n')
        f.write(f'全连接层维度: {fc_hidden}\n')
        f.write(f'Dropout: {dropout}\n')
        f.write(f'学习率: {lr}\n')
        f.write(f'批次大小: {batch_size}\n')
        f.write(f'训练轮数: {epochs}\n')
        f.write(f'SWA开始轮数: {swa_start}\n')
    
    logger.info(f"评估报告已保存到: {eval_report_path}")
    
    return deep_model

def predict_deep_model(model_path, swa_model_path, feature_engineer_path, file_path, use_byte_seq=True, use_image=True):
    """
    使用深度学习模型预测单个文件
    
    Args:
        model_path: 模型文件路径
        swa_model_path: SWA模型路径
        feature_engineer_path: 特征工程器路径
        file_path: 待预测文件路径
        use_byte_seq: 是否使用字节序列特征
        use_image: 是否使用图像特征
    """
    # 加载特征工程器
    feature_engineer = joblib.load(feature_engineer_path)
    
    # 加载标准化器
    scaler_path = os.path.join(os.path.dirname(feature_engineer_path), 'scaler.pkl')
    if os.path.exists(scaler_path):
        scaler = joblib.load(scaler_path)
    else:
        # 如果没有找到标准化器，则使用单位变换
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
    
    # 提取特征
    logger.info(f"从文件提取特征: {file_path}")
    X = load_single_file(file_path)
    
    if X is None:
        logger.error(f"提取特征失败: {file_path}")
        return None
    
    # 提取API序列
    logger.info(f"提取API序列: {file_path}")
    api_sequence = extract_api_sequence(file_path)
    
    # 提取字节序列（如果启用）
    byte_sequence = None
    if use_byte_seq:
        logger.info(f"提取字节序列: {file_path}")
        byte_sequence = extract_bytes_from_file(file_path)
        byte_sequence = np.expand_dims(byte_sequence, axis=0)  # 添加batch维度
    
    # 提取图像特征（如果启用）
    image = None
    if use_image:
        logger.info(f"生成图像特征: {file_path}")
        image = generate_pe_image(file_path)
        image = np.expand_dims(image, axis=0)  # 添加batch维度
    
    # 应用特征工程
    X_fe = feature_engineer.transform(X)
    
    # 应用标准化
    X_scaled = scaler.transform(X_fe)
    
    # 加载模型
    logger.info("加载深度学习模型...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = DeepMalwareDetector.load_model(model_path, swa_model_path, device)
    
    # 预测
    pred, prob = model.predict(
        pe_features=X_scaled,
        api_sequences=[api_sequence],
        use_swa=True,
        byte_sequences=byte_sequence,
        images=image
    )
    
    result = {
        'file': file_path,
        'prediction': 'Malware' if pred[0] else 'Benign',
        'probability': float(prob[0])
    }
    
    logger.info(f"预测结果: {result}")
    return result

def main():
    parser = argparse.ArgumentParser(description='恶意软件检测模型')
    parser.add_argument('--model-type', type=str, choices=['lightgbm', 'deep'], default='lightgbm',
                      help='使用的模型类型: lightgbm (默认) 或 deep (深度学习)')
    
    subparsers = parser.add_subparsers(dest='command', help='命令')
    
    # 训练命令
    train_parser = subparsers.add_parser('train', help='训练恶意软件检测模型')
    train_parser.add_argument('--config', type=str, required=True, help='配置文件路径')
    
    # 预测命令
    predict_parser = subparsers.add_parser('predict', help='预测单个文件')
    predict_parser.add_argument('--model', type=str, required=True, help='模型路径')
    predict_parser.add_argument('--feature-engineer', type=str, required=True, help='特征工程器路径')
    predict_parser.add_argument('--file', type=str, required=True, help='待预测文件路径')
    
    # 深度学习模型特有参数
    predict_parser.add_argument('--swa-model', type=str, help='SWA模型路径 (仅深度学习模型需要)')
    predict_parser.add_argument('--use-byte-seq', action='store_true', default=True, help='是否使用字节序列特征 (仅深度学习模型)')
    predict_parser.add_argument('--use-image', action='store_true', default=True, help='是否使用图像特征 (仅深度学习模型)')
    predict_parser.add_argument('--no-byte-seq', action='store_false', dest='use_byte_seq', help='不使用字节序列特征 (仅深度学习模型)')
    predict_parser.add_argument('--no-image', action='store_false', dest='use_image', help='不使用图像特征 (仅深度学习模型)')
    
    args = parser.parse_args()
    
    if args.command == 'train':
        # 加载配置
        with open(args.config, 'r') as f:
            config = yaml.safe_load(f)
        
        if args.model_type == 'lightgbm':
            train_lightgbm_model(config)
        else:
            train_deep_model(config)
    
    elif args.command == 'predict':
        if args.model_type == 'lightgbm':
            result = predict_lightgbm_file(args.model, args.feature_engineer, args.file)
        else:
            if not args.swa_model:
                logger.error("使用深度学习模型预测时，必须提供 --swa-model 参数")
                return
            
            result = predict_deep_model(
                args.model, 
                args.swa_model, 
                args.feature_engineer, 
                args.file,
                use_byte_seq=args.use_byte_seq,
                use_image=args.use_image
            )
        
        if result:
            print(f"预测结果: {result['prediction']}")
            print(f"恶意概率: {result['probability']:.4f}")
    
    else:
        parser.print_help()

if __name__ == '__main__':
    main() 